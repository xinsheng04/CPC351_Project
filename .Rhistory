sales_matrix[1, 1] <- 340000
sales_matrix[1, 2] <- 560000
sales_matrix[1, 3] <- 654000
sales_matrix[2, 3] <- 683600
sales_matrix[3, 4] <- 684621
sales_matrix
# Build a 3x3 matrix
M <- matrix(c(1,3,3,5,0,4,3,3,3),nrow = 3,ncol = 3)
# Arrays and Matrices 03 =================
library(matrixcalc)
# Build a 3x3 matrix
M <- matrix(c(1,3,3,5,0,4,3,3,3),nrow = 3,ncol = 3)
library(matrixcalc)
# Build a 3x3 matrix
M <- matrix(c(1,3,3,5,0,4,3,3,3),nrow = 3,ncol = 3)
# Print matrix M
print(M)
library(matrixcalc)
# Build a 3x3 matrix
M <- matrix(c(1,3,3,5,0,4,3,3,3),nrow = 3,ncol = 3)
# Print matrix M
print(M)
# Multiply M by inverse(M) to obtain the identity matrix
M %*% matrix.inverse(M)
# Transpose matrix M
t(M)
x <- 3
y <- 8
print(x + y)
for(i in LETTERS){
i <- toupper(i)
if(i=='A' || i == 'E' || i == 'I' || i == 'O' || i == 'U'){
print(paste(i, " is a vowel."))
} else{
print(paste(i, " is a consonant."))
}
}
for(i in letters){
i <- toupper(i)
if(i=='A' || i == 'E' || i == 'I' || i == 'O' || i == 'U'){
print(paste(i, " is a vowel."))
} else{
print(paste(i, " is a consonant."))
}
}
for(i in letters){
j <- toupper(i)
if(j=='A' || j == 'E' || j == 'I' || j == 'O' || j == 'U'){
print(paste(i, " is a vowel."))
} else{
print(paste(i, " is a consonant."))
}
}
calcCircumference <- function(radius) {
circumference <- 2 * pi * radius
return (circumference)
}
calcArea <- function(radius) {
area <- pi * radius * radius
return (area)
}
if(interactive()){
radius <- as.integer(readline(prompt="Enter the radius of the circle: "))
print(paste("The circumference of the circle is ", calcCircumference(radius)))
print(paste("The area of the circle is ", calcArea(radius)))
}
food <- 0
chickens <- 1500
while (chickens >= 300){
food <- food + (chickens * 0.8)
chickens <- chickens - 200
}
print(paste("Food costs: ", food))
food <- 0
chickens <- 1500
while (chickens >= 300){
food <- food + (chickens * 0.8)
chickens <- chickens - 200
}
print(paste("Food costs: RM", food))
foodReducer <- function(chickens, cost, limit){
f <- 0
c<- chickens
while (c >= limit){
f <- f + (c * cost)
c <- c - 200
}
return (f)
}
print(foodReducer(1500, 0.8, 300))
library(here)
# ---------------------------------------------------------------------------- #
# Q1
# Import data using read.table() function
CCS592 <- read.table(here("Q1_CCS592.txt"), header = FALSE, col.names = "Name")
library(here)
library(here)
CCS592 <- read.table(here("Q1_CCS592.txt"), header = FALSE, col.names = "Name")
here::here()
getwd()
getwd()
dset <- read_excel("unemployment_rates.xlsx")
dset <- read_xlsx("unemployment_rates.xlsx")
library(readxl)
dset <- read_xlsx("unemployment_rates.xlsx")
dset <- read_xlsx("unemployment_rates_cs.xlsx")
View(dset)
barplot(height=dset$rate, space=2)
# Load the necessary library
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)
# 1. Create the data frame based on your stats
df <- data.frame(
Country = c("South Korea", "United Kingdom", "United States", "India"),
Rate = c(0.112, 0.097, 0.061, 0.061)
)
# 2. Convert Rate to Percentage for easier viewing
df$Percentage <- df$Rate * 100
# 3. Create the plot
ggplot(df, aes(x = reorder(Country, -Percentage), y = Percentage, fill = Country)) +
geom_bar(stat = "identity", width = 0.7) +
# Add the value labels on top of the bars
geom_text(aes(label = paste0(round(Percentage, 1), "%")),
vjust = -0.5, size = 5, fontface = "bold") +
# Styling and Titles
labs(
title = "Computer Science Fresh Grad Unemployment Rate (2025)",
subtitle = "Outstanding rates compared across major global economies",
x = "Country",
y = "Unemployment Rate (%)",
caption = "Sources: Biswas (2025), Paterson & Paterson (2025), Bank of Korea"
) +
# Clean academic theme
theme_minimal() +
theme(
legend.position = "none", # Hide legend since countries are labeled on x-axis
plot.title = element_text(face = "bold", size = 16),
axis.text = element_text(size = 12),
axis.title = element_text(size = 13)
) +
# Ensure the Y-axis has a bit of extra room for labels
scale_y_continuous(limits = c(0, 13))
# Put your working directories over here
# setwd("C:/Users/Jun & Heng/Desktop/CPC351 R Prgramming/CPC351_Project")
setwd("C:/Users/Asus/OneDrive/Desktop/myProjects/Y3S1/CPC 351/CPC351_Project")
#-----------------------------------------------------------------------------
# This R file tackles the uber dataset,
# performing Data Preprocessing, EDA and model experimentation
# in order to answer the problem statement
#-----------------------------------------------------------------------------
#-----------------------------------------------------------------------------
# Import csv file
#-----------------------------------------------------------------------------
dataset <- read.csv("Project_1_Datasets/uber.csv", header = TRUE)
# Inspect dataset features
cat("Dimensions of the dataset: ", dim(dataset))
# Structure of the dataset
str(dataset)
# Summary of dataset
summary(dataset)
#-----------------------------------------------------------------------------
# Data preprocessing
#-----------------------------------------------------------------------------
# 1. Check for duplicates
print(sum(duplicated(dataset)))
# 2. Check for missing values
for (c in colnames(dataset)) {
cat("Number of missing values in column ", c, " is: ", sum(is.na(dataset[c])), "\n")
}
# 3. Deal with missing and invalid values
narows <- dataset[is.na(dataset$dropoff_longitude) | is.na(dataset$dropoff_latitude), ]
same_location <- dataset[dataset$pickup_longitude == dataset$dropoff_longitude &
dataset$pickup_latitude == dataset$dropoff_latitude, ]
negative_fare <- dataset[dataset$fare_amount < 0, ]
# Remove trips out of NYC
area_outboundaries <- dataset[dataset$dropoff_longitude < -74.05 | dataset$dropoff_longitude > -73.85 |
dataset$pickup_longitude < -74.05 | dataset$pickup_longitude > -73.85 |
dataset$dropoff_latitude < 40.65 | dataset$dropoff_latitude > 40.85 |
dataset$pickup_latitude < 40.65 | dataset$pickup_latitude > 40.85, ]
invalid_rows <- unique(rbind(narows, same_location, negative_fare, area_outboundaries))
exclusionRate <- nrow(invalid_rows) / nrow(dataset)
print(paste("Exclusion rate:", exclusionRate))
# 4. Filter out rows that appear in the invalid_rows table
dataset <- dataset[!(row.names(dataset) %in% row.names(invalid_rows)), ]
# Recheck dataset
print(summary(dataset))
#-----------------------------------------------------------------------------
# K-means model training
#-----------------------------------------------------------------------------
# 1. Extract and derive the relevant features for model classification
# Remove the X and key columns
filtered_dataset <- dataset[, -c(1, 2)]
# Convert to Datetime and extract the Hour
filtered_dataset$pickup_datetime <- as.POSIXct(filtered_dataset$pickup_datetime,
format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
# Create a new column for the hour (0-23)
filtered_dataset$pickup_hour <- as.numeric(format(filtered_dataset$pickup_datetime, "%H"))
# Remove the pickup_datetime and passenger_count column by index
filtered_dataset <- filtered_dataset[, -c(2, 7)]
print(summary(filtered_dataset))
# Get the pickup location and dropoff location information
pickup_location <- filtered_dataset[, c("pickup_latitude", "pickup_longitude",
"dropoff_latitude", "dropoff_longitude")]
# 2. Use Within Sum Squares (WSS) method to determine optimal value of k
max_k <- 20
wss <- numeric(max_k)
for (k in 1:max_k) {
model <- kmeans(pickup_location,
centers = k, nstart = 25,
iter.max = 100, algorithm = "Hartigan-Wong"
)
wss[k] <- sum(model$withinss)
}
plot(1:max_k, wss, main = "WSS over pickup location", type = "b", xlab = "Number of Clusters",
ylab = "Within Sum of Squares")
# 3. Create the kmeans model using the best k values obtained from plotting
k_best <- 5
fit <- kmeans(pickup_location,
centers = k_best, nstart = 25,
iter.max = 100, algorithm = "Hartigan-Wong"
)
# View the model
table(fit$cluster)
# Add the results back to filtered_dataset for reporting
filtered_dataset$pickup_cluster <- as.factor(fit$cluster)
#-----------------------------------------------------------------------------
# Results Diagnosis and Evaluation
#-----------------------------------------------------------------------------
# Plot to see the distribution of data across different clustering models
library("ggplot2")
library("grDevices")
library(leaflet)
library(dplyr)
pickup_clusters <- filtered_dataset[, c("pickup_longitude",
"pickup_latitude", "pickup_cluster")]
dropoff_clusters <- filtered_dataset[, c("dropoff_longitude",
"dropoff_latitude", "pickup_cluster")]
#------------------------
# Plot pickup clusters
#------------------------
# 1. Create the hulls for each zone
hulls <- do.call(rbind, lapply(unique(pickup_clusters$pickup_cluster), function(c) {
# Filter for each specific cluster
df_sub <- pickup_clusters[pickup_clusters$pickup_cluster == c, ]
# Calculate the indices of the convex hull points and return those rows
hull_indices <- chull(df_sub$pickup_longitude, df_sub$pickup_latitude)
return(df_sub[hull_indices, ])
}))
# 2. Create a color palette based on the clusters
pal <- colorFactor(palette = "Set1", domain = pickup_clusters$pickup_cluster)
map <- leaflet() %>%
addProviderTiles(providers$CartoDB.Positron) # A clean, grey map perfect for data
# 3. Add the Polygons
for (c in unique(pickup_clusters$pickup_cluster)) {
hull_data <- hulls %>% filter(pickup_cluster == c)
map <- map %>%
addPolygons(
data = hull_data,
lng = ~pickup_longitude,
lat = ~pickup_latitude,
fillColor = ~pal(c),
weight = 2,
opacity = 1,
fillOpacity = 0.4,
group = paste("Cluster", c),
label = paste("Cluster", c)
)
}
# 4. Add the individual points for detail
map <- map %>%
addCircleMarkers(
data = pickup_clusters,
lng = ~pickup_longitude,
lat = ~pickup_latitude,
radius = 1,
color = ~pal(pickup_cluster),
stroke = FALSE,
fillOpacity = 0.2,
group = "Points"
) %>%
addLayersControl(
overlayGroups = c("Points", paste("Cluster", unique(pickup_clusters$pickup_cluster))),
options = layersControlOptions(collapsed = TRUE)
) %>%
addLegend(
data = pickup_clusters,
position = "topright",
pal = pal,
values = ~pickup_cluster,
title = "Pickup Clusters",
opacity = 1
)
# Group pickup hours based on cluster
#--------------------------------------
hourly_pickup_distribution <- as.data.frame(
table(
pickup_clusters$pickup_cluster,
pickup_clusters$pickup_hour
)
)
View(pickup_clusters)
View(dropoff_clusters)
View(pickup_clusters)
# Put your working directories over here
# setwd("C:/Users/Jun & Heng/Desktop/CPC351 R Prgramming/CPC351_Project")
setwd("C:/Users/Asus/OneDrive/Desktop/myProjects/Y3S1/CPC 351/CPC351_Project")
#-----------------------------------------------------------------------------
# This R file tackles the uber dataset,
# performing Data Preprocessing, EDA and model experimentation
# in order to answer the problem statement
#-----------------------------------------------------------------------------
#-----------------------------------------------------------------------------
# Import csv file
#-----------------------------------------------------------------------------
dataset <- read.csv("Project_1_Datasets/uber.csv", header = TRUE)
# Inspect dataset features
cat("Dimensions of the dataset: ", dim(dataset))
# Structure of the dataset
str(dataset)
# Summary of dataset
summary(dataset)
#-----------------------------------------------------------------------------
# Data preprocessing
#-----------------------------------------------------------------------------
# 1. Check for duplicates
print(sum(duplicated(dataset)))
# 2. Check for missing values
for (c in colnames(dataset)) {
cat("Number of missing values in column ", c, " is: ", sum(is.na(dataset[c])), "\n")
}
# 3. Deal with missing and invalid values
narows <- dataset[is.na(dataset$dropoff_longitude) | is.na(dataset$dropoff_latitude), ]
same_location <- dataset[dataset$pickup_longitude == dataset$dropoff_longitude &
dataset$pickup_latitude == dataset$dropoff_latitude, ]
negative_fare <- dataset[dataset$fare_amount < 0, ]
# Remove trips out of NYC
area_outboundaries <- dataset[dataset$dropoff_longitude < -74.05 | dataset$dropoff_longitude > -73.85 |
dataset$pickup_longitude < -74.05 | dataset$pickup_longitude > -73.85 |
dataset$dropoff_latitude < 40.65 | dataset$dropoff_latitude > 40.85 |
dataset$pickup_latitude < 40.65 | dataset$pickup_latitude > 40.85, ]
invalid_rows <- unique(rbind(narows, same_location, negative_fare, area_outboundaries))
exclusionRate <- nrow(invalid_rows) / nrow(dataset)
print(paste("Exclusion rate:", exclusionRate))
# 4. Filter out rows that appear in the invalid_rows table
dataset <- dataset[!(row.names(dataset) %in% row.names(invalid_rows)), ]
# Recheck dataset
print(summary(dataset))
#-----------------------------------------------------------------------------
# K-means model training
#-----------------------------------------------------------------------------
# 1. Extract and derive the relevant features for model classification
# Remove the X and key columns
filtered_dataset <- dataset[, -c(1, 2)]
# Convert to Datetime and extract the Hour
filtered_dataset$pickup_datetime <- as.POSIXct(filtered_dataset$pickup_datetime,
format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
# Create a new column for the hour (0-23)
filtered_dataset$pickup_hour <- as.numeric(format(filtered_dataset$pickup_datetime, "%H"))
# Remove the pickup_datetime and passenger_count column by index
filtered_dataset <- filtered_dataset[, -c(2, 7)]
print(summary(filtered_dataset))
# Get the pickup location and dropoff location information
pickup_location <- filtered_dataset[, c("pickup_latitude", "pickup_longitude",
"dropoff_latitude", "dropoff_longitude")]
# 2. Use Within Sum Squares (WSS) method to determine optimal value of k
max_k <- 20
wss <- numeric(max_k)
for (k in 1:max_k) {
model <- kmeans(pickup_location,
centers = k, nstart = 25,
iter.max = 100, algorithm = "Hartigan-Wong"
)
wss[k] <- sum(model$withinss)
}
plot(1:max_k, wss, main = "WSS over pickup location", type = "b", xlab = "Number of Clusters",
ylab = "Within Sum of Squares")
# 3. Create the kmeans model using the best k values obtained from plotting
k_best <- 5
fit <- kmeans(pickup_location,
centers = k_best, nstart = 25,
iter.max = 100, algorithm = "Hartigan-Wong"
)
# View the model
table(fit$cluster)
# Add the results back to filtered_dataset for reporting
filtered_dataset$pickup_cluster <- as.factor(fit$cluster)
#-----------------------------------------------------------------------------
# Results Diagnosis and Evaluation
#-----------------------------------------------------------------------------
# Plot to see the distribution of data across different clustering models
library("ggplot2")
library("grDevices")
library(leaflet)
library(dplyr)
pickup_clusters <- filtered_dataset[, c("pickup_longitude", "pickup_latitude",
"pickup_cluster", "pickup_hour")]
dropoff_clusters <- filtered_dataset[, c("dropoff_longitude", "dropoff_latitude",
"pickup_cluster", "pickup_hour")]
#------------------------
# Plot pickup clusters
#------------------------
# 1. Create the hulls for each zone
hulls <- do.call(rbind, lapply(unique(pickup_clusters$pickup_cluster), function(c) {
# Filter for each specific cluster
df_sub <- pickup_clusters[pickup_clusters$pickup_cluster == c, ]
# Calculate the indices of the convex hull points and return those rows
hull_indices <- chull(df_sub$pickup_longitude, df_sub$pickup_latitude)
return(df_sub[hull_indices, ])
}))
# 2. Create a color palette based on the clusters
pal <- colorFactor(palette = "Set1", domain = pickup_clusters$pickup_cluster)
map <- leaflet() %>%
addProviderTiles(providers$CartoDB.Positron) # A clean, grey map perfect for data
# 3. Add the Polygons
for (c in unique(pickup_clusters$pickup_cluster)) {
hull_data <- hulls %>% filter(pickup_cluster == c)
map <- map %>%
addPolygons(
data = hull_data,
lng = ~pickup_longitude,
lat = ~pickup_latitude,
fillColor = ~pal(c),
weight = 2,
opacity = 1,
fillOpacity = 0.4,
group = paste("Cluster", c),
label = paste("Cluster", c)
)
}
# 4. Add the individual points for detail
map <- map %>%
addCircleMarkers(
data = pickup_clusters,
lng = ~pickup_longitude,
lat = ~pickup_latitude,
radius = 1,
color = ~pal(pickup_cluster),
stroke = FALSE,
fillOpacity = 0.2,
group = "Points"
) %>%
addLayersControl(
overlayGroups = c("Points", paste("Cluster", unique(pickup_clusters$pickup_cluster))),
options = layersControlOptions(collapsed = TRUE)
) %>%
addLegend(
data = pickup_clusters,
position = "topright",
pal = pal,
values = ~pickup_cluster,
title = "Pickup Clusters",
opacity = 1
)
print(map)
# Group pickup hours based on cluster
#--------------------------------------
hourly_pickup_distribution <- as.data.frame(
table(
pickup_clusters$pickup_cluster,
pickup_clusters$pickup_hour
)
)
colnames(hourly_pickup_distribution) <- c(
"pickup_cluster", "pickup_hour", "trip_count"
)
# Plotting Bar Charts to show peak hour for each cluster
ggplot(hourly_pickup_distribution, aes(
x = as.numeric(as.character(pickup_hour)),
y = trip_count, fill = pickup_cluster
)) +
geom_bar(stat = "identity") +
facet_wrap(~pickup_cluster, nrow = 2, ncol = 4, scales = "free_y") +
scale_x_continuous(breaks = seq(0, 23, by = 4)) +
labs(
title = "Peak Hours by Pickup Cluster",
x = "Hour of Day (0â€“23)",
y = "Number of Pickups"
) +
theme_minimal() +
theme(legend.position = "none", panel.spacing.y = unit(2, "lines"))
#------------------------
# Plot dropoff clusters
#------------------------
# 1. Create the hulls for each zone
hulls <- do.call(rbind, lapply(unique(dropoff_clusters$pickup_cluster), function(c) {
# Filter for each specific cluster
df_sub <- dropoff_clusters[dropoff_clusters$pickup_cluster == c, ]
# Calculate the indices of the convex hull points and return those rows
hull_indices <- chull(df_sub$pickup_longitude, df_sub$pickup_latitude)
return(df_sub[hull_indices, ])
}))
# 2. Create a color palette based on the clusters
pal <- colorFactor(palette = "Set1", domain = dropoff_clusters$pickup_cluster)
map <- leaflet() %>%
addProviderTiles(providers$CartoDB.Positron) # A clean, grey map perfect for data
# 3. Add the Polygons
for (c in unique(dropoff_clusters$pickup_cluster)) {
hull_data <- hulls %>% filter(pickup_cluster == c)
map <- map %>%
addPolygons(
data = hull_data,
lng = ~pickup_longitude,
lat = ~pickup_latitude,
fillColor = ~pal(c),
weight = 2,
opacity = 1,
fillOpacity = 0.4,
group = paste("Cluster", c),
label = paste("Cluster", c)
)
}
View(filtered_dataset)
