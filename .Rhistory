cluster_3_cols <- c("Previous_Purchases", "Average_Spending", "Last_Purchase_Days_Ago",
"Browsing_Time_Before_Purchase", "Customer_Satisfaction_Rating", shopping_time_cols)
remove_cols <- c("Customer_ID", "Transaction_ID", "Product_ID")
subset_df <- dataset_scale[, setdiff(colnames(dataset_scale), remove_cols)]
subset_1_df <- subset_df[, cluster_1_cols]
subset_2_df <- subset_df[, cluster_2_cols]
subset_3_df <- subset_df[, cluster_3_cols]
# 2. Use Within Sum Squares (WSS) method to determine optimal value of k
max_k <- 20
wss_1 <- numeric(max_k)
for (k in 1:max_k){
wss_1[k] <- sum(kmeans(subset_1_df, centers=k, nstart=25)$withinss)
}
plot(1:max_k, wss_1, main="WSS over customer backgrounds", type="b", xlab="Number of Clusters", ylab="Within Sum of Squares")
wss_2 <- numeric(max_k)
for (k in 1:max_k){
wss_2[k] <- sum(kmeans(subset_2_df, centers=k, nstart=25)$withinss)
}
plot(1:max_k, wss_2, main="WSS over purchase trends", type="b", xlab="Number of Clusters", ylab="Within Sum of Squares")
wss_3 <- numeric(max_k)
for (k in 1:max_k){
wss_3[k] <- sum(kmeans(subset_3_df, centers=k, nstart=25)$withinss)
}
plot(1:max_k, wss_3, main="WSS over shopping trends", type="b", xlab="Number of Clusters", ylab="Within Sum of Squares")
# 3. Create the kmeans model using the best k values obtained from plotting
k_best_1 <- 4
k_best_2 <- 5
K_best_3 <- 4
fit1 <- kmeans(subset_1_df, centers=k_best_1, nstart=25)
fit2 <- kmeans(subset_2_df, centers=k_best_2, nstart=25)
fit3 <- kmeans(subset_2_df, centers=k_best_2, nstart=25)
# View the model
table(fit1$cluster)
table(fit2$cluster)
table(fit3$cluster)
# Add the results back to the original dataset for reporting
dataset$Cluster_Demographics <- as.factor(fit1$cluster)
dataset$Cluster_Spending     <- as.factor(fit2$cluster)
dataset$Cluster_Engagement   <- as.factor(fit3$cluster)
# Plot to see the distribution of data across different clustering models
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Demographics)
plot(dataset[, c("Average_Spending", "Loyalty_Points_Used")], col=dataset$Cluster_Spending)
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Engagement)
# Profile each cluster for data interpretation
profile_1 <- aggregate(dataset[, cluster_1_cols],
by = list(Cluster = dataset$Cluster_Demographics),
FUN=mean)
profile_2 <- aggregate(dataset[, cluster_2_cols],
by = list(Cluster = dataset$Cluster_Spending),
FUN=mean)
profile_3 <- aggregate(dataset[, cluster_3_cols],
by = list(Cluster = dataset$Cluster_Engagement),
FUN=mean)
View(profile_1)
View(profile_2)
View(profile_3)
# Put your working directories over here
setwd("C:/Users/Asus/OneDrive/Desktop/myProjects/Y3S1/CPC 351/CPC351_Project")
#-----------------------------------------------------------------------------
# This R file tackles the real_estate dataset,
# performing Data Preprocessing, EDA and model experimentation
# in order to answer the problem statement
#-----------------------------------------------------------------------------
#-----------------------------------------------------------------------------
# Import csv file
#-----------------------------------------------------------------------------
library(readxl)
dataset <- read_excel("Project_1_Datasets/Customer Purchase Behavior datasets.xlsx")
# Inspect dataset features
cat("Dimensions of the dataset: ", dim(dataset))
# Structure of the dataset
str(dataset)
# Summary of dataset
summary(dataset)
# Observe examples from the dataset
head(dataset)
dataset[sample(nrow(dataset), size=5, replace=TRUE),]
#-----------------------------------------------------------------------------
# Data preprocessing
#-----------------------------------------------------------------------------
#1. Check for duplicates
print(sum(duplicated(dataset)))
print(sum(duplicated(dataset$Transaction_ID)))
#2. Check for missing values
for(c in colnames(dataset)){
cat("Number of missing values in column ", c, " is: ", sum(is.na(dataset[c])), "\n")
}
#3. Deal with missing values
# View columns with income = 0
naIncome <- dataset[is.na(dataset$Income),]
# Set missing values for Income column to 0
dataset$Income <- ifelse(is.na(dataset$Income), 0, dataset$Income)
# Remove other rows that have missing values
dataset <- dataset[complete.cases(dataset),]
# Recheck dataset
print(summary(dataset))
#4. Data normalization
# Apply One-hot Encoding (OHE) onto nominal data
cols_to_OHE <- c("Gender", "Location", "Customer_Segment", "Product_Category",
"Payment_Method", "Discount_Applied", "Preferred_Shopping_Time")
for(col in cols_to_OHE){
ohe_matrix <- model.matrix(as.formula(paste0("~ ", col, " - 1")), data = dataset)
ohe_df <- as.data.frame(ohe_matrix)
dataset <- cbind(dataset, ohe_df)
}
# Scale columns
numeric_cols <- sapply(dataset, is.numeric)
cols_to_scale <- names(dataset)[numeric_cols]
# Create a copy of the dataset for scaling
dataset_scale <- dataset
dataset_scale[, cols_to_scale] <- scale(dataset[, cols_to_scale])
#4. Visualize frequency distributions
library(ggplot2)
#-----------------------------------------------------------------------------
# Feature selection & Model training
#-----------------------------------------------------------------------------
# 1. Subset dataset to create datasets for unsupervised learning
# We are clustering using different feature sets
# Demographics
# Set 1: Age, Gender, Income, Location, Customer_Segment
# Goal: Determine distribution of products across customer backgrounds,
#       Determine use of discounts across different demographics
# Spending
# Set 2: Quantity, Total_Spent, Discount_Applied, Loyalty_Points_Used
# Goal: Determine distribution of products across purchase trends
#       Determine distribution of purchase times across purchase trends
# Engagement
# Set 3: Previous_Purchases, Average_Spending, Last_Purchase_Days_Ago
# Browsing_Time_Before_Purchase, Preferred_Shopping_Time, Customer_Satisfaction_Rating
# Goal: Determine distribution of products across shopping trends
#       Determine distribution of customers across shopping trends
#function to get all OHE columns
getOHECols <- function (colname){
ohecols <- grep(colname, colnames(dataset_scale), value=TRUE)
# Remove the first entry, which is the original column of OHE
ohecols <- ohecols[ohecols != colname]
return(ohecols)
}
gender_cols <- getOHECols("Gender")
location_cols <- getOHECols("Location")
customer_segment_cols <- getOHECols("Customer_Segment")
discount_cols <- getOHECols("Discount_Applied")
shopping_time_cols <- getOHECols("Preferred_Shopping_Time")
cluster_1_cols <- c("Age", Income", gender_cols, location_cols, customer_segment_cols)
cluster_2_cols <- c("Quantity", "Total_Spent", "Loyalty_Points_Used", discount_cols)
# Put your working directories over here
setwd("C:/Users/Asus/OneDrive/Desktop/myProjects/Y3S1/CPC 351/CPC351_Project")
#-----------------------------------------------------------------------------
# This R file tackles the real_estate dataset,
# performing Data Preprocessing, EDA and model experimentation
# in order to answer the problem statement
#-----------------------------------------------------------------------------
#-----------------------------------------------------------------------------
# Import csv file
#-----------------------------------------------------------------------------
library(readxl)
dataset <- read_excel("Project_1_Datasets/Customer Purchase Behavior datasets.xlsx")
# Inspect dataset features
cat("Dimensions of the dataset: ", dim(dataset))
# Structure of the dataset
str(dataset)
# Summary of dataset
summary(dataset)
# Observe examples from the dataset
head(dataset)
dataset[sample(nrow(dataset), size=5, replace=TRUE),]
#-----------------------------------------------------------------------------
# Data preprocessing
#-----------------------------------------------------------------------------
#1. Check for duplicates
print(sum(duplicated(dataset)))
print(sum(duplicated(dataset$Transaction_ID)))
#2. Check for missing values
for(c in colnames(dataset)){
cat("Number of missing values in column ", c, " is: ", sum(is.na(dataset[c])), "\n")
}
#3. Deal with missing values
# View columns with income = 0
naIncome <- dataset[is.na(dataset$Income),]
# Set missing values for Income column to 0
dataset$Income <- ifelse(is.na(dataset$Income), 0, dataset$Income)
# Remove other rows that have missing values
dataset <- dataset[complete.cases(dataset),]
# Recheck dataset
print(summary(dataset))
#4. Data normalization
# Apply One-hot Encoding (OHE) onto nominal data
cols_to_OHE <- c("Gender", "Location", "Customer_Segment", "Product_Category",
"Payment_Method", "Discount_Applied", "Preferred_Shopping_Time")
for(col in cols_to_OHE){
ohe_matrix <- model.matrix(as.formula(paste0("~ ", col, " - 1")), data = dataset)
ohe_df <- as.data.frame(ohe_matrix)
dataset <- cbind(dataset, ohe_df)
}
# Scale columns
numeric_cols <- sapply(dataset, is.numeric)
cols_to_scale <- names(dataset)[numeric_cols]
# Create a copy of the dataset for scaling
dataset_scale <- dataset
dataset_scale[, cols_to_scale] <- scale(dataset[, cols_to_scale])
#4. Visualize frequency distributions
library(ggplot2)
#-----------------------------------------------------------------------------
# Feature selection & Model training
#-----------------------------------------------------------------------------
# 1. Subset dataset to create datasets for unsupervised learning
# We are clustering using different feature sets
# Demographics
# Set 1: Age, Gender, Income, Location, Customer_Segment
# Goal: Determine distribution of products across customer backgrounds,
#       Determine use of discounts across different demographics
# Spending
# Set 2: Quantity, Total_Spent, Discount_Applied, Loyalty_Points_Used
# Goal: Determine distribution of products across purchase trends
#       Determine distribution of purchase times across purchase trends
# Engagement
# Set 3: Previous_Purchases, Average_Spending, Last_Purchase_Days_Ago
# Browsing_Time_Before_Purchase, Preferred_Shopping_Time, Customer_Satisfaction_Rating
# Goal: Determine distribution of products across shopping trends
#       Determine distribution of customers across shopping trends
#function to get all OHE columns
getOHECols <- function (colname){
ohecols <- grep(colname, colnames(dataset_scale), value=TRUE)
# Remove the first entry, which is the original column of OHE
ohecols <- ohecols[ohecols != colname]
return(ohecols)
}
gender_cols <- getOHECols("Gender")
location_cols <- getOHECols("Location")
customer_segment_cols <- getOHECols("Customer_Segment")
discount_cols <- getOHECols("Discount_Applied")
shopping_time_cols <- getOHECols("Preferred_Shopping_Time")
cluster_1_cols <- c("Age", "Income", gender_cols, location_cols, customer_segment_cols)
cluster_2_cols <- c("Quantity", "Total_Spent", "Loyalty_Points_Used", discount_cols)
cluster_3_cols <- c("Previous_Purchases", "Average_Spending", "Last_Purchase_Days_Ago",
"Browsing_Time_Before_Purchase", "Customer_Satisfaction_Rating", shopping_time_cols)
remove_cols <- c("Customer_ID", "Transaction_ID", "Product_ID")
subset_df <- dataset_scale[, setdiff(colnames(dataset_scale), remove_cols)]
subset_1_df <- subset_df[, cluster_1_cols]
subset_2_df <- subset_df[, cluster_2_cols]
subset_3_df <- subset_df[, cluster_3_cols]
# 2. Use Within Sum Squares (WSS) method to determine optimal value of k
max_k <- 20
wss_1 <- numeric(max_k)
for (k in 1:max_k){
wss_1[k] <- sum(kmeans(subset_1_df, centers=k, nstart=25)$withinss)
}
plot(1:max_k, wss_1, main="WSS over customer backgrounds", type="b", xlab="Number of Clusters", ylab="Within Sum of Squares")
wss_2 <- numeric(max_k)
for (k in 1:max_k){
wss_2[k] <- sum(kmeans(subset_2_df, centers=k, nstart=25)$withinss)
}
plot(1:max_k, wss_2, main="WSS over purchase trends", type="b", xlab="Number of Clusters", ylab="Within Sum of Squares")
wss_3 <- numeric(max_k)
for (k in 1:max_k){
wss_3[k] <- sum(kmeans(subset_3_df, centers=k, nstart=25)$withinss)
}
plot(1:max_k, wss_3, main="WSS over shopping trends", type="b", xlab="Number of Clusters", ylab="Within Sum of Squares")
# 3. Create the kmeans model using the best k values obtained from plotting
k_best_1 <- 4
k_best_2 <- 5
K_best_3 <- 4
fit1 <- kmeans(subset_1_df, centers=k_best_1, nstart=25)
fit2 <- kmeans(subset_2_df, centers=k_best_2, nstart=25)
fit3 <- kmeans(subset_2_df, centers=k_best_2, nstart=25)
# View the model
table(fit1$cluster)
table(fit2$cluster)
table(fit3$cluster)
# Add the results back to the original dataset for reporting
dataset$Cluster_Demographics <- as.factor(fit1$cluster)
dataset$Cluster_Spending     <- as.factor(fit2$cluster)
dataset$Cluster_Engagement   <- as.factor(fit3$cluster)
# Plot to see the distribution of data across different clustering models
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Demographics)
plot(dataset[, c("Average_Spending", "Loyalty_Points_Used")], col=dataset$Cluster_Spending)
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Engagement)
# Profile each cluster for data interpretation
profile_1 <- aggregate(dataset[, cluster_1_cols],
by = list(Cluster = dataset$Cluster_Demographics),
FUN=mean)
profile_2 <- aggregate(dataset[, cluster_2_cols],
by = list(Cluster = dataset$Cluster_Spending),
FUN=mean)
profile_3 <- aggregate(dataset[, cluster_3_cols],
by = list(Cluster = dataset$Cluster_Engagement),
FUN=mean)
# 3. Create the kmeans model using the best k values obtained from plotting
k_best_1 <- 6
k_best_2 <- 5
K_best_3 <- 4
fit1 <- kmeans(subset_1_df, centers=k_best_1, nstart=25)
fit2 <- kmeans(subset_2_df, centers=k_best_2, nstart=25)
fit3 <- kmeans(subset_2_df, centers=k_best_2, nstart=25)
# View the model
table(fit1$cluster)
table(fit2$cluster)
table(fit3$cluster)
# Add the results back to the original dataset for reporting
dataset$Cluster_Demographics <- as.factor(fit1$cluster)
dataset$Cluster_Spending     <- as.factor(fit2$cluster)
dataset$Cluster_Engagement   <- as.factor(fit3$cluster)
# Plot to see the distribution of data across different clustering models
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Demographics)
plot(dataset[, c("Average_Spending", "Loyalty_Points_Used")], col=dataset$Cluster_Spending)
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Engagement)
# Profile each cluster for data interpretation
profile_1 <- aggregate(dataset[, cluster_1_cols],
by = list(Cluster = dataset$Cluster_Demographics),
FUN=mean)
profile_2 <- aggregate(dataset[, cluster_2_cols],
by = list(Cluster = dataset$Cluster_Spending),
FUN=mean)
profile_3 <- aggregate(dataset[, cluster_3_cols],
by = list(Cluster = dataset$Cluster_Engagement),
FUN=mean)
View(profile_1)
# Put your working directories over here
setwd("C:/Users/Asus/OneDrive/Desktop/myProjects/Y3S1/CPC 351/CPC351_Project")
#-----------------------------------------------------------------------------
# This R file tackles the real_estate dataset,
# performing Data Preprocessing, EDA and model experimentation
# in order to answer the problem statement
#-----------------------------------------------------------------------------
#-----------------------------------------------------------------------------
# Import csv file
#-----------------------------------------------------------------------------
library(readxl)
dataset <- read_excel("Project_1_Datasets/Customer Purchase Behavior datasets.xlsx")
# Inspect dataset features
cat("Dimensions of the dataset: ", dim(dataset))
# Structure of the dataset
str(dataset)
# Summary of dataset
summary(dataset)
# Observe examples from the dataset
head(dataset)
dataset[sample(nrow(dataset), size=5, replace=TRUE),]
#-----------------------------------------------------------------------------
# Data preprocessing
#-----------------------------------------------------------------------------
#1. Check for duplicates
print(sum(duplicated(dataset)))
print(sum(duplicated(dataset$Transaction_ID)))
#2. Check for missing values
for(c in colnames(dataset)){
cat("Number of missing values in column ", c, " is: ", sum(is.na(dataset[c])), "\n")
}
#3. Deal with missing values
# View columns with income = 0
naIncome <- dataset[is.na(dataset$Income),]
# Set missing values for Income column to 0
dataset$Income <- ifelse(is.na(dataset$Income), 0, dataset$Income)
# Remove other rows that have missing values
dataset <- dataset[complete.cases(dataset),]
# Recheck dataset
print(summary(dataset))
#4. Data normalization
# Apply One-hot Encoding (OHE) onto nominal data
cols_to_OHE <- c("Gender", "Location", "Customer_Segment", "Product_Category",
"Payment_Method", "Discount_Applied", "Preferred_Shopping_Time")
for(col in cols_to_OHE){
ohe_matrix <- model.matrix(as.formula(paste0("~ ", col, " - 1")), data = dataset)
ohe_df <- as.data.frame(ohe_matrix)
dataset <- cbind(dataset, ohe_df)
}
# Scale columns
numeric_cols <- sapply(dataset, is.numeric)
cols_to_scale <- names(dataset)[numeric_cols]
# Create a copy of the dataset for scaling
dataset_scale <- dataset
dataset_scale[, cols_to_scale] <- scale(dataset[, cols_to_scale])
#4. Visualize frequency distributions
library(ggplot2)
#-----------------------------------------------------------------------------
# Feature selection & Model training
#-----------------------------------------------------------------------------
# 1. Subset dataset to create datasets for unsupervised learning
# We are clustering using different feature sets
# Demographics
# Set 1: Age, Gender, Income, Location, Customer_Segment
# Goal: Determine distribution of products across customer backgrounds,
#       Determine use of discounts across different demographics
# Spending
# Set 2: Quantity, Total_Spent, Discount_Applied, Loyalty_Points_Used
# Goal: Determine distribution of products across purchase trends
#       Determine distribution of purchase times across purchase trends
# Engagement
# Set 3: Previous_Purchases, Average_Spending, Last_Purchase_Days_Ago
# Browsing_Time_Before_Purchase, Preferred_Shopping_Time, Customer_Satisfaction_Rating
# Goal: Determine distribution of products across shopping trends
#       Determine distribution of customers across shopping trends
#function to get all OHE columns
getOHECols <- function (colname){
ohecols <- grep(colname, colnames(dataset_scale), value=TRUE)
# Remove the first entry, which is the original column of OHE
ohecols <- ohecols[ohecols != colname]
return(ohecols)
}
gender_cols <- getOHECols("Gender")
location_cols <- getOHECols("Location")
customer_segment_cols <- getOHECols("Customer_Segment")
discount_cols <- getOHECols("Discount_Applied")
shopping_time_cols <- getOHECols("Preferred_Shopping_Time")
cluster_1_cols <- c("Age", "Income", gender_cols, location_cols, customer_segment_cols)
cluster_2_cols <- c("Quantity", "Total_Spent", "Loyalty_Points_Used", discount_cols)
cluster_3_cols <- c("Previous_Purchases", "Average_Spending", "Last_Purchase_Days_Ago",
"Browsing_Time_Before_Purchase", "Customer_Satisfaction_Rating", shopping_time_cols)
remove_cols <- c("Customer_ID", "Transaction_ID", "Product_ID")
subset_df <- dataset_scale[, setdiff(colnames(dataset_scale), remove_cols)]
subset_1_df <- subset_df[, cluster_1_cols]
subset_2_df <- subset_df[, cluster_2_cols]
subset_3_df <- subset_df[, cluster_3_cols]
# 2. Use Within Sum Squares (WSS) method to determine optimal value of k
max_k <- 20
wss_1 <- numeric(max_k)
for (k in 1:max_k){
wss_1[k] <- sum(kmeans(subset_1_df, centers=k, nstart=25)$withinss)
}
plot(1:max_k, wss_1, main="WSS over customer backgrounds", type="b", xlab="Number of Clusters", ylab="Within Sum of Squares")
wss_2 <- numeric(max_k)
for (k in 1:max_k){
wss_2[k] <- sum(kmeans(subset_2_df, centers=k, nstart=25)$withinss)
}
plot(1:max_k, wss_2, main="WSS over purchase trends", type="b", xlab="Number of Clusters", ylab="Within Sum of Squares")
wss_3 <- numeric(max_k)
for (k in 1:max_k){
wss_3[k] <- sum(kmeans(subset_3_df, centers=k, nstart=25)$withinss)
}
plot(1:max_k, wss_3, main="WSS over shopping trends", type="b", xlab="Number of Clusters", ylab="Within Sum of Squares")
# Plot to see the distribution of data across different clustering models
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Demographics)
plot(dataset[, c("Average_Spending", "Loyalty_Points_Used")], col=dataset$Cluster_Spending)
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Engagement)
# Profile each cluster for data interpretation
profile_1 <- aggregate(dataset[, cluster_1_cols],
by = list(Cluster = dataset$Cluster_Demographics),
FUN=mean)
View(dataset)
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Demographics)
data = dataset, FUN = mean)
length(dataset[, cluster_1_cols])
#-----------------------------------------------------------------------------
# Results Diagnosis and Evaluation
#-----------------------------------------------------------------------------
# Plot to see the distribution of data across different clustering models
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Demographics)
plot(dataset[, c("Average_Spending", "Loyalty_Points_Used")], col=dataset$Cluster_Spending)
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Engagement)
# Profile each cluster for data interpretation
profile_1 <- aggregate(dataset[, cluster_1_cols],
by = list(Cluster = dataset$Cluster_Demographics),
FUN=mean)
#-----------------------------------------------------------------------------
# Results Diagnosis and Evaluation
#-----------------------------------------------------------------------------
# Plot to see the distribution of data across different clustering models
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Demographics)
plot(dataset[, c("Average_Spending", "Loyalty_Points_Used")], col=dataset$Cluster_Spending)
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Engagement)
# Profile each cluster for data interpretation
profile_1 <- aggregate(dataset[, cluster_1_cols],
by = list(Cluster = dataset$Cluster_Demographics),
FUN=mean)
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Demographics)
plot(dataset[, c("Average_Spending", "Loyalty_Points_Used")], col=dataset$Cluster_Spending)
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Engagement)
colnames(dataset)
# 3. Create the kmeans model using the best k values obtained from plotting
k_best_1 <- 6
k_best_2 <- 5
K_best_3 <- 4
fit1 <- kmeans(subset_1_df, centers=k_best_1, nstart=25)
fit2 <- kmeans(subset_2_df, centers=k_best_2, nstart=25)
fit3 <- kmeans(subset_2_df, centers=k_best_2, nstart=25)
# View the model
table(fit1$cluster)
table(fit2$cluster)
table(fit3$cluster)
# Add the results back to the original dataset for reporting
dataset$Cluster_Demographics <- as.factor(fit1$cluster)
dataset$Cluster_Spending     <- as.factor(fit2$cluster)
dataset$Cluster_Engagement   <- as.factor(fit3$cluster)
#-----------------------------------------------------------------------------
# Results Diagnosis and Evaluation
#-----------------------------------------------------------------------------
# Plot to see the distribution of data across different clustering models
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Demographics)
plot(dataset[, c("Average_Spending", "Loyalty_Points_Used")], col=dataset$Cluster_Spending)
plot(dataset[, c("Income", "Total_Spent")], col=dataset$Cluster_Engagement)
# Profile each cluster for data interpretation
profile_1 <- aggregate(dataset[, cluster_1_cols],
by = list(Cluster = dataset$Cluster_Demographics),
FUN=mean)
profile_2 <- aggregate(dataset[, cluster_2_cols],
by = list(Cluster = dataset$Cluster_Spending),
FUN=mean)
profile_3 <- aggregate(dataset[, cluster_3_cols],
by = list(Cluster = dataset$Cluster_Engagement),
FUN=mean)
View(profile_1)
for(c in colnames(dataset)){
cat("Number of missing values in column ", c, " is: ", sum(is.na(dataset[c])), "\n")
}
summary(dataset$Gender)
str(dataset$Gender)
unique(dataset$Gender)
View(profile_1)
View(profile_2)
View(profile_2)
View(profile_3)
View(profile_3)
