print(summary(subset_df))
# Scale the data
subset_df <- scale(subset_df)
# 2. Use Within Sum Squares (WSS) method to determine optimal value of k
max_k <- 20
wss <- numeric(max_k)
for (k in 1:max_k){
wss[k] <- sum(kmeans(subset_df, centers=k, nstart=25)$withinss)
}
plot(1:max_k, wss, type="b", xlab="Number of Clusters", ylab="Within Sum of Squares")
install.packages(factoextra)
install.packages("factoextra")
library(factoextra)
fviz_nbclust(subset_df, kmeans, method = "silhouette")
cols_to_remove <- c("ID", "Square_Feet" "Has_Garden_Factor", "Has_Pool_Factor", "Price")
cols_to_remove <- c("ID", "Square_Feet", "Has_Garden_Factor", "Has_Pool_Factor", "Price")
subset_df <- dataset[, setdiff(colnames(dataset), cols_to_remove)]
print(summary(subset_df))
# Scale the data
subset_df <- scale(subset_df)
# 2. Use Within Sum Squares (WSS) method to determine optimal value of k
max_k <- 20
wss <- numeric(max_k)
for (k in 1:max_k){
wss[k] <- sum(kmeans(subset_df, centers=k, nstart=25)$withinss)
}
plot(1:max_k, wss, type="b", xlab="Number of Clusters", ylab="Within Sum of Squares")
fviz_nbclust(subset_df, kmeans, method = "silhouette")
cols_to_remove <- c("ID", "Square_Feet", "Location_Score", "Distance_to_Center", "Has_Garden_Factor", "Has_Pool_Factor", "Price")
subset_df <- dataset[, setdiff(colnames(dataset), cols_to_remove)]
print(summary(subset_df))
# Scale the data
subset_df <- scale(subset_df)
# 2. Use Within Sum Squares (WSS) method to determine optimal value of k
max_k <- 20
wss <- numeric(max_k)
for (k in 1:max_k){
wss[k] <- sum(kmeans(subset_df, centers=k, nstart=25)$withinss)
}
plot(1:max_k, wss, type="b", xlab="Number of Clusters", ylab="Within Sum of Squares")
fviz_nbclust(subset_df, kmeans, method = "silhouette")
setwd("C:/Users/Jun & Heng/Desktop/CPC351 R Prgramming/CPC351_Project")
library(readxl)
dataset <- read_excel("Project_1_Datasets/Customer Purchase Behavior datasets.xlsx")
# Inspect dataset features
cat("Dimensions of the dataset: ", dim(dataset))
View(dataset)
# Structure of the dataset
str(dataset)
# Summary of dataset
summary(dataset)
# Observe examples from the dataset
head(dataset)
dataset[sample(nrow(dataset), size = 5, replace = TRUE), ]
print(sum(duplicated(dataset)))
print(sum(duplicated(dataset$Transaction_ID)))
for (c in colnames(dataset)) {
cat("Number of missing values in column ", c, " is: ", sum(is.na(dataset[c])), "\n")
}
naIncome <- dataset[is.na(dataset$Income), ]
naIncome
dataset$Income <- ifelse(is.na(dataset$Income), 0, dataset$Income)
dataset <- dataset[complete.cases(dataset), ]
print(summary(dataset))
# Apply One-hot Encoding (OHE) onto nominal data
cols_to_OHE <- c(
"Gender", "Location", "Customer_Segment", "Product_Category",
"Payment_Method", "Discount_Applied", "Preferred_Shopping_Time"
)
for (col in cols_to_OHE) {
ohe_matrix <- model.matrix(as.formula(paste0("~ ", col, " - 1")), data = dataset)
ohe_df <- as.data.frame(ohe_matrix)
dataset <- cbind(dataset, ohe_df)
}
numeric_cols <- sapply(dataset, is.numeric)
cols_to_scale <- names(dataset)[numeric_cols]
# Create a copy of the dataset for scaling
dataset_scale <- dataset
dataset_scale[, cols_to_scale] <- scale(dataset[, cols_to_scale])
getOHECols <- function(colname) {
ohecols <- grep(colname, colnames(dataset_scale), value = TRUE)
# Remove the first entry, which is the original column of OHE
ohecols <- ohecols[ohecols != colname]
return(ohecols)
}
gender_cols <- getOHECols("Gender")
location_cols <- getOHECols("Location")
customer_segment_cols <- getOHECols("Customer_Segment")
discount_cols <- getOHECols("Discount_Applied")
shopping_time_cols <- getOHECols("Preferred_Shopping_Time")
cluster_1_cols <- c("Age", "Income", gender_cols, location_cols, customer_segment_cols)
cluster_2_cols <- c("Quantity", "Total_Spent", "Loyalty_Points_Used", discount_cols)
cluster_3_cols <- c(
"Previous_Purchases", "Average_Spending", "Last_Purchase_Days_Ago",
"Browsing_Time_Before_Purchase", "Customer_Satisfaction_Rating", shopping_time_cols
)
remove_cols <- c("Customer_ID", "Transaction_ID", "Product_ID")
subset_df <- dataset_scale[, setdiff(colnames(dataset_scale), remove_cols)]
subset_1_df <- subset_df[, cluster_1_cols]
subset_2_df <- subset_df[, cluster_2_cols]
subset_3_df <- subset_df[, cluster_3_cols]
# 2. Use Within Sum Squares (WSS) method to determine optimal value of k
max_k <- 20
wss_1 <- numeric(max_k)
for (k in 1:max_k) {
wss_1[k] <- sum(kmeans(subset_1_df, centers = k, nstart = 25)$withinss)
}
plot(1:max_k, wss_1, main = "WSS over customer backgrounds", type = "b", xlab = "Number of Clusters", ylab = "Within Sum of Squares")
wss_2 <- numeric(max_k)
for (k in 1:max_k) {
wss_2[k] <- sum(kmeans(subset_2_df, centers = k, nstart = 25)$withinss)
}
plot(1:max_k, wss_2, main = "WSS over purchase trends", type = "b", xlab = "Number of Clusters", ylab = "Within Sum of Squares")
wss_3 <- numeric(max_k)
for (k in 1:max_k) {
wss_3[k] <- sum(kmeans(subset_3_df, centers = k, nstart = 25)$withinss)
}
plot(1:max_k, wss_3, main = "WSS over shopping trends", type = "b", xlab = "Number of Clusters", ylab = "Within Sum of Squares")
# 3. Create the kmeans model using the best k values obtained from plotting
k_best_1 <- 6
k_best_2 <- 5
K_best_3 <- 4
fit1 <- kmeans(subset_1_df, centers = k_best_1, nstart = 25)
fit2 <- kmeans(subset_2_df, centers = k_best_2, nstart = 25)
fit3 <- kmeans(subset_2_df, centers = k_best_2, nstart = 25)
table(fit1$cluster)
table(fit2$cluster)
table(fit3$cluster)
# Add the results back to the original dataset for reporting
dataset$Cluster_Demographics <- as.factor(fit1$cluster)
dataset$Cluster_Spending <- as.factor(fit2$cluster)
dataset$Cluster_Engagement <- as.factor(fit3$cluster)
plot(dataset[, c("Income", "Total_Spent")], col = dataset$Cluster_Demographics)
plot(dataset[, c("Average_Spending", "Loyalty_Points_Used")], col = dataset$Cluster_Spending)
plot(dataset[, c("Income", "Total_Spent")], col = dataset$Cluster_Engagement)
# Profile each cluster for data interpretation
profile_1 <- aggregate(dataset[, cluster_1_cols],
by = list(Cluster = dataset$Cluster_Demographics),
FUN = mean
)
profile_2 <- aggregate(dataset[, cluster_2_cols],
by = list(Cluster = dataset$Cluster_Spending),
FUN = mean
)
profile_3 <- aggregate(dataset[, cluster_3_cols],
by = list(Cluster = dataset$Cluster_Engagement),
FUN = mean
)
View(profile_1)
setwd("C:/Users/Jun & Heng/Desktop/CPC351 R Prgramming/CPC351_Project")
#-----------------------------------------------------------------------------
# Import csv file
#-----------------------------------------------------------------------------
library(readxl)
dataset <- read_excel("Project_1_Datasets/Customer Purchase Behavior datasets.xlsx")
# Inspect dataset features
cat("Dimensions of the dataset: ", dim(dataset))
# Structure of the dataset
str(dataset)
# Summary of dataset
summary(dataset)
# Observe examples from the dataset
head(dataset)
dataset[sample(nrow(dataset), size = 5, replace = TRUE), ]
print(sum(duplicated(dataset)))
print(sum(duplicated(dataset$Transaction_ID)))
for (c in colnames(dataset)) {
cat("Number of missing values in column ", c, " is: ", sum(is.na(dataset[c])), "\n")
}
# 3. Deal with missing values
# View columns with income = 0
naIncome <- dataset[is.na(dataset$Income), ]
# Set missing values for Income column to 0
dataset$Income <- ifelse(is.na(dataset$Income), 0, dataset$Income)
# Remove other rows that have missing values
dataset <- dataset[complete.cases(dataset), ]
# Recheck dataset
print(summary(dataset))
# Apply One-hot Encoding (OHE) onto nominal data
cols_to_OHE <- c(
"Gender", "Location", "Customer_Segment", "Product_Category",
"Payment_Method", "Discount_Applied", "Preferred_Shopping_Time"
)
for (col in cols_to_OHE) {
ohe_matrix <- model.matrix(as.formula(paste0("~ ", col, " - 1")), data = dataset)
ohe_df <- as.data.frame(ohe_matrix)
dataset <- cbind(dataset, ohe_df)
}
# Scale columns
numeric_cols <- sapply(dataset, is.numeric)
cols_to_scale <- names(dataset)[numeric_cols]
# Create a copy of the dataset for scaling
dataset_scale <- dataset
dataset_scale[, cols_to_scale] <- scale(dataset[, cols_to_scale])
# function to get all OHE columns
getOHECols <- function(colname) {
ohecols <- grep(colname, colnames(dataset_scale), value = TRUE)
# Remove the first entry, which is the original column of OHE
ohecols <- ohecols[ohecols != colname]
return(ohecols)
}
gender_cols <- getOHECols("Gender")
location_cols <- getOHECols("Location")
customer_segment_cols <- getOHECols("Customer_Segment")
discount_cols <- getOHECols("Discount_Applied")
shopping_time_cols <- getOHECols("Preferred_Shopping_Time")
cluster_1_cols <- c("Age", "Income", gender_cols, location_cols, customer_segment_cols)
cluster_2_cols <- c("Quantity", "Total_Spent", "Loyalty_Points_Used", discount_cols)
cluster_3_cols <- c(
"Previous_Purchases", "Average_Spending", "Last_Purchase_Days_Ago",
"Browsing_Time_Before_Purchase", "Customer_Satisfaction_Rating", shopping_time_cols
)
remove_cols <- c("Customer_ID", "Transaction_ID", "Product_ID")
subset_df <- dataset_scale[, setdiff(colnames(dataset_scale), remove_cols)]
subset_1_df <- subset_df[, cluster_1_cols]
subset_2_df <- subset_df[, cluster_2_cols]
subset_3_df <- subset_df[, cluster_3_cols]
# 2. Use Within Sum Squares (WSS) method to determine optimal value of k
max_k <- 20
wss_1 <- numeric(max_k)
for (k in 1:max_k) {
wss_1[k] <- sum(kmeans(subset_1_df, centers = k, nstart = 25)$withinss)
}
plot(1:max_k, wss_1, main = "WSS over customer backgrounds", type = "b", xlab = "Number of Clusters", ylab = "Within Sum of Squares")
wss_2 <- numeric(max_k)
for (k in 1:max_k) {
wss_2[k] <- sum(kmeans(subset_2_df, centers = k, nstart = 25)$withinss)
}
plot(1:max_k, wss_2, main = "WSS over purchase trends", type = "b", xlab = "Number of Clusters", ylab = "Within Sum of Squares")
wss_3 <- numeric(max_k)
for (k in 1:max_k) {
wss_3[k] <- sum(kmeans(subset_3_df, centers = k, nstart = 25)$withinss)
}
plot(1:max_k, wss_3, main = "WSS over shopping trends", type = "b", xlab = "Number of Clusters", ylab = "Within Sum of Squares")
# 3. Create the kmeans model using the best k values obtained from plotting
k_best_1 <- 6
k_best_2 <- 5
K_best_3 <- 4
fit1 <- kmeans(subset_1_df, centers = k_best_1, nstart = 25)
fit2 <- kmeans(subset_2_df, centers = k_best_2, nstart = 25)
fit3 <- kmeans(subset_2_df, centers = k_best_2, nstart = 25)
# View the model
table(fit1$cluster)
table(fit2$cluster)
table(fit3$cluster)
# Add the results back to the original dataset for reporting
dataset$Cluster_Demographics <- as.factor(fit1$cluster)
dataset$Cluster_Spending <- as.factor(fit2$cluster)
dataset$Cluster_Engagement <- as.factor(fit3$cluster)
#-----------------------------------------------------------------------------
# Results Diagnosis and Evaluation
#-----------------------------------------------------------------------------
# Plot to see the distribution of data across different clustering models
plot(dataset[, c("Income", "Total_Spent")], col = dataset$Cluster_Demographics)
plot(dataset[, c("Average_Spending", "Loyalty_Points_Used")], col = dataset$Cluster_Spending)
plot(dataset[, c("Income", "Total_Spent")], col = dataset$Cluster_Engagement)
# Profile each cluster for data interpretation
profile_1 <- aggregate(dataset[, cluster_1_cols],
by = list(Cluster = dataset$Cluster_Demographics),
FUN = mean
)
profile_2 <- aggregate(dataset[, cluster_2_cols],
by = list(Cluster = dataset$Cluster_Spending),
FUN = mean
)
profile_3 <- aggregate(dataset[, cluster_3_cols],
by = list(Cluster = dataset$Cluster_Engagement),
FUN = mean
)
View(dataset)
# Put your working directories over here
setwd("C:/Users/Jun & Heng/Desktop/CPC351 R Prgramming/CPC351_Project")
#-----------------------------------------------------------------------------
# Import csv file
#-----------------------------------------------------------------------------
dataset <- read.csv("Project_1_Datasets/uber.csv", header=TRUE)
View(dataset)
# Inspect dataset features
cat("Dimensions of the dataset: ", dim(dataset))
# Structure of the dataset
str(dataset)
# Summary of dataset
summary(dataset)
head(dataset)
# 1. Check for duplicates
print(sum(duplicated(dataset)))
# 2. Check for missing values
for (c in colnames(dataset)) {
cat("Number of missing values in column ", c, " is: ", sum(is.na(dataset[c])), "\n")
}
nalatitude <- dataset[is.na(dataset$dropoff_longitude), ]
View(nalatitude)
# 3. View columns with income = 0
# Corrected version
narows <- dataset[is.na(dataset$dropoff_longitude) | is.na(dataset$dropoff_latitude), ]
View(nalatitude)
same_location <- dataset[dataset$pickup_longitude == dataset$dropoff_longitude &
dataset$pickup_latitude == dataset$dropoff_latitude, ]
View(same_location)
invalidProbability <- (nrow(narows) + nrow(same_location)) / nrow(dataset)
print("Invalid probabilty", invalidProbability)
print(paste("Invalid probability:", invalidProbability))
dataset <- dataset[!(is.na(dataset$dropoff_longitude) | is.na(dataset$dropoff_latitude)) &
!(dataset$pickup_longitude == dataset$dropoff_longitude &
dataset$pickup_latitude == dataset$dropoff_latitude), ]
for (c in colnames(dataset)) {
cat("Number of missing values in column ", c, " is: ", sum(is.na(dataset[c])), "\n")
}
# Recheck dataset
print(summary(dataset))
# Remove the first column by index
dataset_new <- dataset[, -1]
View(dataset)
View(dataset_new)
dataset_new <- dataset[, -c(1, 2)]
View(dataset_new)
# Calculate Euclidean Distance
dataset_new$dist_euclidean <- sqrt(
(dataset_new$dropoff_longitude - dataset_new$pickup_longitude)^2 +
(dataset_new$dropoff_latitude - dataset_new$pickup_latitude)^2
)
# Convert Datetime and extract Hour
# Convert to formal datetime
dataset_new$pickup_datetime <- as.POSIXct(dataset_new$pickup_datetime, format="%Y-%m-%d %H:%M:%S", tz="UTC")
# Create a new column for the hour (0-23)
dataset_new$pickup_hour <- as.numeric(format(dataset_new$pickup_datetime, "%H"))
# Remove the first column by index
dataset_new <- dataset[, -c(2, 7)]
# Put your working directories over here
setwd("C:/Users/Jun & Heng/Desktop/CPC351 R Prgramming/CPC351_Project")
#-----------------------------------------------------------------------------
# This R file tackles the uber dataset,
# performing Data Preprocessing, EDA and model experimentation
# in order to answer the problem statement
#-----------------------------------------------------------------------------
#-----------------------------------------------------------------------------
# Import csv file
#-----------------------------------------------------------------------------
dataset <- read.csv("Project_1_Datasets/uber.csv", header=TRUE)
# Inspect dataset features
cat("Dimensions of the dataset: ", dim(dataset))
# Structure of the dataset
str(dataset)
# Summary of dataset
summary(dataset)
#-----------------------------------------------------------------------------
# Data preprocessing
#-----------------------------------------------------------------------------
# 1. Check for duplicates
print(sum(duplicated(dataset)))
# 2. Check for missing values
for (c in colnames(dataset)) {
cat("Number of missing values in column ", c, " is: ", sum(is.na(dataset[c])), "\n")
}
# 3. Deal with missing and invalid values
narows <- dataset[is.na(dataset$dropoff_longitude) | is.na(dataset$dropoff_latitude), ]
same_location <- dataset[dataset$pickup_longitude == dataset$dropoff_longitude &
dataset$pickup_latitude == dataset$dropoff_latitude, ]
invalidProbability <- (nrow(narows) + nrow(same_location)) / nrow(dataset)
print(paste("Invalid probability:", invalidProbability))
# 4. Filter out the NAs and the Same Location rows
dataset <- dataset[!(is.na(dataset$dropoff_longitude) | is.na(dataset$dropoff_latitude)) &
!(dataset$pickup_longitude == dataset$dropoff_longitude &
dataset$pickup_latitude == dataset$dropoff_latitude), ]
# Recheck dataset
print(summary(dataset))
# Remove the first column by index
dataset_new <- dataset[, -c(1, 2)]
# Calculate Euclidean Distance
dataset_new$dist_euclidean <- sqrt(
(dataset_new$dropoff_longitude - dataset_new$pickup_longitude)^2 +
(dataset_new$dropoff_latitude - dataset_new$pickup_latitude)^2
)
# Convert Datetime and extract Hour
# Convert to formal datetime
dataset_new$pickup_datetime <- as.POSIXct(dataset_new$pickup_datetime, format="%Y-%m-%d %H:%M:%S", tz="UTC")
# Create a new column for the hour (0-23)
dataset_new$pickup_hour <- as.numeric(format(dataset_new$pickup_datetime, "%H"))
# Remove the first column by index
dataset_new <- dataset_new[, -c(2, 7)]
View(dataset_new)
print(summary(dataset_new))
negative_fare <- dataset[dataset$fare_amount < 0]
negative_fare <- dataset[dataset$fare_amount < 0, ]
View(negative_fare)
invalidProbability <- (nrow(narows) + nrow(same_location) + nrow(negative_fare)) / nrow(dataset)
print(paste("Invalid probability:", invalidProbability))
# Put your working directories over here
setwd("C:/Users/Jun & Heng/Desktop/CPC351 R Prgramming/CPC351_Project")
#-----------------------------------------------------------------------------
# This R file tackles the uber dataset,
# performing Data Preprocessing, EDA and model experimentation
# in order to answer the problem statement
#-----------------------------------------------------------------------------
#-----------------------------------------------------------------------------
# Import csv file
#-----------------------------------------------------------------------------
dataset <- read.csv("Project_1_Datasets/uber.csv", header=TRUE)
# Inspect dataset features
cat("Dimensions of the dataset: ", dim(dataset))
# Structure of the dataset
str(dataset)
# Summary of dataset
summary(dataset)
#-----------------------------------------------------------------------------
# Data preprocessing
#-----------------------------------------------------------------------------
# 1. Check for duplicates
print(sum(duplicated(dataset)))
# 2. Check for missing values
for (c in colnames(dataset)) {
cat("Number of missing values in column ", c, " is: ", sum(is.na(dataset[c])), "\n")
}
# 3. Deal with missing and invalid values
narows <- dataset[is.na(dataset$dropoff_longitude) | is.na(dataset$dropoff_latitude), ]
same_location <- dataset[dataset$pickup_longitude == dataset$dropoff_longitude &
dataset$pickup_latitude == dataset$dropoff_latitude, ]
negative_fare <- dataset[dataset$fare_amount < 0, ]
invalidProbability <- (nrow(narows) + nrow(same_location) + nrow(negative_fare)) / nrow(dataset)
print(paste("Invalid probability:", invalidProbability))
# 4. Filter out the NAs and the Same Location rows
dataset <- dataset[!(is.na(dataset$dropoff_longitude) | is.na(dataset$dropoff_latitude)) &
!(dataset$fare_amount < 0) &
!(dataset$pickup_longitude == dataset$dropoff_longitude &
dataset$pickup_latitude == dataset$dropoff_latitude), ]
# Recheck dataset
print(summary(dataset))
# Remove the first column by index
dataset_new <- dataset[, -c(1, 2)]
# Calculate Euclidean Distance
dataset_new$dist_euclidean <- sqrt(
(dataset_new$dropoff_longitude - dataset_new$pickup_longitude)^2 +
(dataset_new$dropoff_latitude - dataset_new$pickup_latitude)^2
)
# Convert Datetime and extract Hour
# Convert to formal datetime
dataset_new$pickup_datetime <- as.POSIXct(dataset_new$pickup_datetime, format="%Y-%m-%d %H:%M:%S", tz="UTC")
# Create a new column for the hour (0-23)
dataset_new$pickup_hour <- as.numeric(format(dataset_new$pickup_datetime, "%H"))
# Remove the pickup_datetime and passenger_count column by index
dataset_new <- dataset_new[, -c(2, 7)]
print(summary(dataset_new))
# 2. Check for missing values
for (c in colnames(dataset)) {
cat("Number of missing values in column ", c, " is: ", sum(is.na(dataset[c])), "\n")
}
View(dataset_new)
pickup_location <- filtered_dataset[,c("pickup_latitude", "pickup_longitude")]
# Put your working directories over here
setwd("C:/Users/Jun & Heng/Desktop/CPC351 R Prgramming/CPC351_Project")
#-----------------------------------------------------------------------------
# This R file tackles the uber dataset,
# performing Data Preprocessing, EDA and model experimentation
# in order to answer the problem statement
#-----------------------------------------------------------------------------
#-----------------------------------------------------------------------------
# Import csv file
#-----------------------------------------------------------------------------
dataset <- read.csv("Project_1_Datasets/uber.csv", header = TRUE)
# Inspect dataset features
cat("Dimensions of the dataset: ", dim(dataset))
# Structure of the dataset
str(dataset)
# Summary of dataset
summary(dataset)
#-----------------------------------------------------------------------------
# Data preprocessing
#-----------------------------------------------------------------------------
# 1. Check for duplicates
print(sum(duplicated(dataset)))
# 2. Check for missing values
for (c in colnames(dataset)) {
cat("Number of missing values in column ", c, " is: ", sum(is.na(dataset[c])), "\n")
}
# 3. Deal with missing and invalid values
narows <- dataset[is.na(dataset$dropoff_longitude) | is.na(dataset$dropoff_latitude), ]
same_location <- dataset[dataset$pickup_longitude == dataset$dropoff_longitude &
dataset$pickup_latitude == dataset$dropoff_latitude, ]
negative_fare <- dataset[dataset$fare_amount < 0, ]
invalidProbability <- (nrow(narows) + nrow(same_location) + nrow(negative_fare)) / nrow(dataset)
print(paste("Invalid probability:", invalidProbability))
# 4. Filter out the NAs and the Same Location rows
dataset <- dataset[!(is.na(dataset$dropoff_longitude) | is.na(dataset$dropoff_latitude)) &
!(dataset$fare_amount < 0) &
!(dataset$pickup_longitude == dataset$dropoff_longitude &
dataset$pickup_latitude == dataset$dropoff_latitude), ]
# Recheck dataset
print(summary(dataset))
# Remove the first column by index
filtered_dataset <- dataset[, -c(1, 2)]
# Calculate Euclidean Distance
filtered_dataset$dist_euclidean <- sqrt(
(filtered_dataset$dropoff_longitude - filtered_dataset$pickup_longitude)^2 +
(filtered_dataset$dropoff_latitude - filtered_dataset$pickup_latitude)^2
)
# Convert Datetime and extract Hour
# Convert to formal datetime
filtered_dataset$pickup_datetime <- as.POSIXct(filtered_dataset$pickup_datetime, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
# Create a new column for the hour (0-23)
filtered_dataset$pickup_hour <- as.numeric(format(filtered_dataset$pickup_datetime, "%H"))
# Remove the pickup_datetime and passenger_count column by index
filtered_dataset <- filtered_dataset[, -c(2, 7)]
print(summary(filtered_dataset))
pickup_location <- filtered_dataset[,c("pickup_latitude", "pickup_longitude")]
View(pickup_location)
dropoff_location <- filtered_dataset[,c("dropoff_latitude", "dropoff_longitude")]
View(dropoff_location)
max_k <- 20
wss_1 <- numeric(max_k)
for (k in 1:max_k) {
wss_1[k] <- sum(kmeans(pickup_location, centers = k, nstart = 25)$withinss)
}
